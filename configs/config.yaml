# AI-Prepping Configuration

# Model settings
models:
  llm:
    default: "llama3.2"
    alternatives:
      - "mistral"
      - "qwen2.5"
      - "phi3"
      - "gemma2"
    # Ollama API settings
    api_url: "http://localhost:11434"
    max_tokens: 2048
    temperature: 0.7
    top_p: 0.95
    timeout: 120
  
  embeddings:
    model: "nomic-embed-text"
    dimension: 768
    max_length: 512
    # Fallback to sentence-transformers if Ollama embedding fails
    fallback_model: "sentence-transformers/all-MiniLM-L6-v2"

# Data paths
paths:
  models_dir: "./models"
  data_dir: "./data"
  index_dir: "./index"
  cache_dir: "./cache"
  history_file: "./cache/.ai-prepping_history"

# Wikipedia settings
wikipedia:
  dump_url: "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"
  language: "en"
  max_articles: null  # null for all articles
  chunk_size: 512
  chunk_overlap: 128

# Retrieval settings
retrieval:
  top_k: 5
  score_threshold: 0.7
  max_context_length: 4096
  rerank: true

# System settings
system:
  max_memory_gb: 32
  low_memory_mode: false  # Auto-enabled if <20GB available
  num_threads: 8
  device: "mps"  # Metal Performance Shaders
  
# Download settings
downloads:
  chunk_size: 8192
  timeout: 300
  max_retries: 3
  verify_checksums: true

# UI settings
ui:
  cli:
    prompt: "ðŸ¤– AI-Prepper> "
    color_scheme: "monokai"
    enable_history: true
    max_history: 1000
  
  web:
    host: "127.0.0.1"
    port: 8000
    debug: false

# Logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: null  # Set to path for file logging

# Model checksums (SHA-256)
checksums:
  "Mistral-7B-v0.1-8bit": "sha256:expected_hash_here"
  "Mistral-7B-v0.1-4bit": "sha256:expected_hash_here"
  "bge-large-en-v1.5": "sha256:expected_hash_here"